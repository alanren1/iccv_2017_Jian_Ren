%****************************************************************
\section{Experiments}

In this section, we present the experimental evaluation of our personalized aesthetics model (PAM) and our active learning approach (Active-PAM) on both the FLICKR-AES and the REAL-CUR datasets.
Figure~\ref{examples} introduces visual examples to show how the model works. 
\subsection{Implementation details}
%\paragraph{Implementation}
 The earlier layers of our generic aesthetics network are initialized from the Inception-BN network \cite{ioffe2015batch}, whereas the last trimmed inception module is randomly initialized using ``Xavier''. The network is then fine-tuned on FLICKR-AES. During training, images are warped to $256 \times 256$ and then randomly cropped to $224 \times 224$ before feeding into the network. The learning rate is initialized as 0.001 and periodically annealed by 0.96. The weight decay is 0.0002 and the momentum is 0.9. 


%\paragraph{Evaluation results}
%The evaluation of generic model is measured by the Spearman's rank correlation ($\rho$) \cite{myers2010research} between predicted aesthetics scores and the ground-truth values. The correlation is calculated as $\rho = 1 - 6\frac{\sum_{i=1}^{N}{(r_i - r_{i}^{'})}^2}{N^3 - N}$, where $r_i$ is the rank of the $i$-th item when sorting the ground truth scores from high to low and $r_{i}^{'}$ is the rank for the estimated score. $\rho$ ranges from -1 to 1 and higher value indicates better correlation between the predicted scores and the ground-truth scores. The correlation for the aesthetics generic model is 0.702. Table~\ref{attributes} shows the correlation on ten attributes. Interestingly, we found that the prediction of the attributes is significantly better than the one from\cite{kong2016photo}, benefiting from larger training dataset in generic model.

\subsection{Evaluation on personalized aesthetics models}

To evaluate our personalized aesthetics model (PAM), we compare PAM with a baseline model as well as a commonly used collaborate filtering approach and show that PAM works significantly better than the other two. We further analyze the effectiveness of content and aesthetic attribute features used in our model.


\paragraph{Comparison with other methods.} We compare PAM with two other methods: 1) a baseline SVM regressor that directly predicts user-specific aesthetics scores on test images based on user-provided training images, and 2) a commonly used collaborate filtering approach, non-linear Feature-based Matrix Factorization (FPMF), which achieves better performance than other matrix factorization approaches on individual color aesthetics recommendation\cite{o2014collaborative}. In both methods, we use the same content and aesthetic attribute features as in our method. 



%The non-linear FPMF trains a one layer neural network to map the image features from training set into latent space, therefore it can predict user's preference on the images that are not contained in the training set\cite{o2014collaborative}. The one layer neural network in non-linear FPMF includes 200 logistic units that can be trained by back-propagation and the dimensionality for the latent space is 15. 

We evaluate these three methods on the test workers in FLICKR-AES. During evaluation, for each test worker, we randomly sample $k$ images from the ones he or she labeled, and use them as training images. All the remaining images are then used for testing. For the non-linear FPMF, all the other workers in the FLICKR-AES training set are also included for training. Due to the randomness of training image selection, we run the experiments 50 times for each test worker, and report the averaged results as well as the standard deviation. 

Following \cite{kong2016photo}, ranking correlations are used to measure the consistency between the predictions and the ground-truth user scores. The mean ranking correlation of the generic aesthetics model over all the test workers are 0.514. In Table~\ref{mfNfeature}, we show the improvement in terms of correlation for each method compared with the generic model, with $k = 10$ and $k = 100$, respectively. We can see the SVM model that directly predict scores does not work on this problem, as its results are even worse than the ones from the generic model. It tries to directly learn each user's flavor regarding aesthetics from very limited data without considering generic aesthetics understanding, which is accordingly very unstable and hardly generalizable. By contrast, our method (last row in Table~\ref{mfNfeature}) works even with 10 training images, and has much more significant improvement with more training examples. It validates the design of our residual-based model, which fully leverages the common understanding of aesthetics existing in the generic aesthetics network, and focuses on the score offsets that directly correspond to users' unique preference compared with generic aesthetics. Our model also significantly outperforms FPMF, which only has marginal improvement even when using 100 training images.

\paragraph{Ablation study on features.}  We also show the results of PAM when trained only using the content feature or only using the aesthetics attribute feature, respectively, in the 3rd and 4th row in Table~\ref{mfNfeature}. We can see that both content and aesthetics attributes can be used to model personalized aesthetics, as the correlations with users' ground truth scores also increase  when using more user provided training images. Nevertheless, using both features gives the best performance. It further demonstrates that users' preference on image aesthetics are related to both image content and aesthetic attributes.


\subsection{Evaluation on active learning}

%\paragraph{Visual vector for training set selection}As described in Section~\ref{Active}, we use the aesthetic features to calculate the aesthetic dissimilarity between images and select images for active learning instead of using other features to encode image content and style information, (e.g, the content feature and attributes feature used to learn the personalized aesthetics model). We show the results using different features in active learning as in  Figure~\ref{5b}. The comparison demonstrates using generic aesthetics score and aesthetics features to select training samples is better than using attributes and contents features, as aesthetics features could better capture the diversity of images. We also show the results of using different criteria for the Active-PAM in Figure~\ref{5b}, which demonstrate using two criteria  (diversity and informativeness) is better than just considering one criteria.



\paragraph{Comparison with other methods.} We compare our method with three other active learning methods: 1) Greedy\cite{yu2010passive}, which selects the sample that has the minimum largest distance to the already selected samples at every iteration;  2) MCAL\cite{demir2014multiple}, which chooses samples by clustering the candidate images to be selected and selected images that are not support vectors;  3) Query by Committee (QBC) \cite{burbidge2007active}, which generates a committee of models by using ensemble algorithms. In our experiment, we generate 5 committees using Bagging for QBC \cite{mamitsuka1998query}. We choose these three methods for comparison because they deal with regression problem, whereas classic active learning approaches\cite{tong2001support1, demir2014multiple} for classification are not applicable here, as their criteria such as margin-based sampling are not suitable for continuous aesthetics scores. In addition, we also add another baseline where all the images are randomly selected.

To evaluate an active learning method, we start with 10 randomly selected images to train the initial PAM. Based on the initial results, we keep selecting new images from user's photo album using the active learning methods and updating the PAM model, until the number of selected images reaches 100. Different methods may select different images for model update. To compare these methods on the same test images, we chose to use the entire photo album for evaluation. It is also consistent with the real application scenario of personalized aesthetics, where the algorithm is able to access and actively select any image in the photo album, and the quality of overall ranking on all images in the album is the most important factor for the user.

Due to the randomness in the initialization of the PAM model, we repeat the experiments 10 times for each method, and measure the average performance. The results are shown in Figure~\ref{fig:testresults}. Our active selection method outperforms all the other baseline methods as well as random selection.

To further examine the generalizablility of the PAM models updated by different active learning approaches, instead of evaluating on the entire photo album, we remove all the images that have already been selected for model update, and only evaluate on the remaining images the model has not seen before.  We note that it is not a totally fair comparison, as images used for evaluation may be different for different active learning approaches, due to the different images they selected for model update. But it still gives us a sense how the model works on new images. The results are reported in Figure~\ref{fig:testresults2}. When evaluating on those unseen images, the PAM model updated by our active selection performs significantly better.
%\vspace{-0.1in}
%\subsection{Generalization study}
%\vspace{-0.1in}
%Moreover, the Active-PAM can be applied to any generic aesthetic model to accommodate user preference. To validate the generalizability of Active-PAM, we use the network from \cite{kong2016photo} as the generic aesthetics model instead of our own trained network, and test the results of personalized image aesthetics. The reason for using the model developed in \cite{kong2016photo} instead of other generic models is that the model is trained on the dataset (AADB) that collected from natural images and with rater identity. So we can run the experiment on the same dataset by splitting images according to rater identity. We use 28 workers in AADB who label 100 to 200 images to test the performance. The result is shown in Figure~\ref{shuModel_fig}, which indicates Active-PAM can be applied to different generic models for learning individual user's aesthetics model. The improvement is less significant than the one using our trained generic aesthetics network. It indicates that a  better generic aesthetics model could help improve personalized image aesthetics more.

